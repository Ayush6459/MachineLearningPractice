{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en import STOP_WORDS\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"A potential issue with this encoder-decoder approach is that a neural network needs to be able to compress all the necessary information of a source sentence into a fixed-length vector. \n",
    "        This may make it difficult for the neural network to cope with long sentences. The performance of a basic encoder-decoder deteriorates rapidly as the length of an input sentence increases.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords=list(STOP_WORDS)\n",
    "pun=punctuation+'\\n'\n",
    "pun=list(pun)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'potential', 'issue', 'with', 'this', 'encoder', '-', 'decoder', 'approach', 'is', 'that', 'a', 'neural', 'network', 'needs', 'to', 'be', 'able', 'to', 'compress', 'all', 'the', 'necessary', 'information', 'of', 'a', 'source', 'sentence', 'into', 'a', 'fixed', '-', 'length', 'vector', '.', '\\n        ', 'This', 'may', 'make', 'it', 'difficult', 'for', 'the', 'neural', 'network', 'to', 'cope', 'with', 'long', 'sentences', '.', 'The', 'performance', 'of', 'a', 'basic', 'encoder', '-', 'decoder', 'deteriorates', 'rapidly', 'as', 'the', 'length', 'of', 'an', 'input', 'sentence', 'increases', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens=[token.text for token in doc]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['potential', 'issue', 'encoder', 'decoder', 'approach', 'neural', 'network', 'needs', 'able', 'compress', 'necessary', 'information', 'source', 'sentence', 'fixed', 'length', 'vector', '\\n        ', 'difficult', 'neural', 'network', 'cope', 'long', 'sentences', 'performance', 'basic', 'encoder', 'decoder', 'deteriorates', 'rapidly', 'length', 'input', 'sentence', 'increases']\n"
     ]
    }
   ],
   "source": [
    "tokens=[w.lower() for w in tokens if w not in stopwords and w not in pun]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENS=Tokenizer()\n",
    "TOKENS.fit_on_texts(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count max frequencies of words\n",
    "max_freq=max(TOKENS.word_counts.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalise the word frequencies \n",
    "\n",
    "for word in TOKENS.word_counts:\n",
    "    TOKENS.word_counts[word]=TOKENS.word_counts[word]/max_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('potential', 0.5), ('issue', 0.5), ('encoder', 1.0), ('decoder', 1.0), ('approach', 0.5), ('neural', 1.0), ('network', 1.0), ('needs', 0.5), ('able', 0.5), ('compress', 0.5), ('necessary', 0.5), ('information', 0.5), ('source', 0.5), ('sentence', 1.0), ('fixed', 0.5), ('length', 1.0), ('vector', 0.5), ('difficult', 0.5), ('cope', 0.5), ('long', 0.5), ('sentences', 0.5), ('performance', 0.5), ('basic', 0.5), ('deteriorates', 0.5), ('rapidly', 0.5), ('input', 0.5), ('increases', 0.5)])\n"
     ]
    }
   ],
   "source": [
    "print(TOKENS.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[A potential issue with this encoder-decoder approach is that a neural network needs to be able to compress all the necessary information of a source sentence into a fixed-length vector. \n",
      "        , This may make it difficult for the neural network to cope with long sentences., The performance of a basic encoder-decoder deteriorates rapidly as the length of an input sentence increases.]\n"
     ]
    }
   ],
   "source": [
    "sentences=[sent for sent in doc.sents]\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_scores={}\n",
    "for sent in sentences:\n",
    "    for word in sent:\n",
    "        if word.text.lower() in TOKENS.word_counts.keys():\n",
    "            if sent not in sentence_scores.keys():\n",
    "                sentence_scores[sent]=TOKENS.word_counts[word.text.lower()]\n",
    "            else:\n",
    "                sentence_scores[sent]+=TOKENS.word_counts[word.text.lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{A potential issue with this encoder-decoder approach is that a neural network needs to be able to compress all the necessary information of a source sentence into a fixed-length vector. \n",
       "         : 11.5,\n",
       " This may make it difficult for the neural network to cope with long sentences.: 4.0,\n",
       " The performance of a basic encoder-decoder deteriorates rapidly as the length of an input sentence increases.: 7.0}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from heapq import nlargest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here i select 40% of total sentences length\n",
    "# we can select as much of our criteria\n",
    "select_length=int(len(sentences)*0.4)\n",
    "select_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[A potential issue with this encoder-decoder approach is that a neural network needs to be able to compress all the necessary information of a source sentence into a fixed-length vector. \n",
       "         ]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary=nlargest(select_length,sentence_scores,key=sentence_scores.get)\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A potential issue with this encoder-decoder approach is that a neural network needs to be able to compress all the necessary information of a source sentence into a fixed-length vector. \\n        '"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finale_summary=''.join([w.text for w in summary])\n",
    "finale_summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python37664bitbaseconda96d6f791f7a948bea67cd3c890e8b7fb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
